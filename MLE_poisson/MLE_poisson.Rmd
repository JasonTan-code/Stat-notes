---
title: "MLE for glm"
author: "Taotao"
date: "10/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=TRUE, warning=FALSE, message=FALSE)
```


### Simulate 6 data point, with two groups.

The population parameter $\beta_0 = 1, \beta_1 = 1.5$
```{r}
x = c(rep(0, 3), rep(1, 3))
y = c()
b0 = 1
b1 = 1.5

for (i in 1:length(x)){
  y[i] <- rpois(1,exp(b1 * x[i] + b0))
}

plot(x, y)

cbind(x, y)
```

### Use MLE to build the model

We need to estimate $\beta_0, \beta_1$. The assumption is that $Y \sim Poisson(e^{\beta_0 + \beta_1x})$

We have the $pmf = \frac{\lambda ^k e^{-k}}{k!}$, where lambda is a parameter (mean of the population) and k is a random variable.

So have the log likelihood of a single point is$ll = log[pmf] = k(b_0 + b_1x) - exp(b_0 + b_1x) - log(k!)$. Plug y into k, and plug x into x, and sum up all data.


```{r}
ll <- function (z){
  b0 <- z[1]
  b1 <- z[2]
  
  return(-(
    y[1]*(b0 + b1*x[1]) - exp(b0 + b1*x[1]) - log(factorial(y[1])) + 
    y[2]*(b0 + b1*x[2]) - exp(b0 + b1*x[2]) - log(factorial(y[2])) +
    y[3]*(b0 + b1*x[3]) - exp(b0 + b1*x[3]) - log(factorial(y[3])) +
    y[4]*(b0 + b1*x[4]) - exp(b0 + b1*x[4]) - log(factorial(y[4])) +
    y[5]*(b0 + b1*x[5]) - exp(b0 + b1*x[5]) - log(factorial(y[5])) +
    y[6]*(b0 + b1*x[6]) - exp(b0 + b1*x[6]) - log(factorial(y[6]))))
}

# here is the optimal parameter
res = optim(c(1,1), ll)

# estimated parameters
# the population parameter is b0 = 1, b1 = 1.5, which is close to the estimated parameters
res$par

# log likelihood
-res$value
```


### validate with glm function; notice the coefficient is the same.
```{r}
glm(y ~ x, family = poisson(link = "log"))
```

Note: The selection of distribution and link function is somewhat arbitrary. For example, I can also use negative binomial as the distribution. If that is the case, I will need to estimate one more parameter: dispersion. But the framework is the same. Conclusion: carefully select model and link function.

### Manually calculate the p value with likelihood ratio test.

Here we want to test if $\hat \beta_1$ is statistically significant. We will perform a likelihood ratio test.

We have already calculated the $log (likelihood)_{max}$ = -optim(c(1,1), ll)$value. Let's build another model where we only have one variable. Again, we will use MLE.

Here $ll_{null} = log[pmf] = k(b_0) - exp(b_0) - log(k!)$

```{r}
ll_null <- function (z){
  b0 <- z[1]
  
  return(-(
    y[1]*(b0) - exp(b0) - log(factorial(y[1])) + 
    y[2]*(b0) - exp(b0) - log(factorial(y[2])) +
    y[3]*(b0) - exp(b0) - log(factorial(y[3])) +
    y[4]*(b0) - exp(b0) - log(factorial(y[4])) +
    y[5]*(b0) - exp(b0) - log(factorial(y[5])) +
    y[6]*(b0) - exp(b0) - log(factorial(y[6]))))
}


null_res = optim(c(1), ll_null)

# estimated parameter
null_res$par

# log likelihood
-null_res$value
```

The formula to calculate the likelihood ratio test is $\lambda = -2[l_{null} - l_{max}]$, where $\lambda$ is chi square distributed with 1 degree of freedom (2 parameters - 1 parameter = 1 df). Then we use pchisq to calculate the area under the curve
```{r}
chi = -2 * (-null_res$value - (-res$value))

1 - pchisq(chi, df =1)
```

## Validate the result with lrtest function. Notice the p value is the same 

summary() calculate a z score for the glm model, which I don't think make too much sense. But the p value is not significantly different from each other (likelihood ratio test vs z test).
```{r}
library(lmtest)
model_null = glm(y~1, poisson(link = "log"))
model = glm(y~x,poisson(link = "log"))

lrtest(model_null,model)

```


I don't think z test make sense for glm.
```{r}
summary(model)$coefficients
```

